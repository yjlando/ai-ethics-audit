# ðŸ§  AI Ethics Audit â€“ COMPAS Case Study  
---

## ðŸ“Œ Project Overview

This project audits **algorithmic bias** in the COMPAS Recidivism Risk dataset using **IBMâ€™s AI Fairness 360 (AIF360)** toolkit. Our goal is to identify unfair outcomes across racial groups, apply remediation techniques, and evaluate fairness using ethical AI metrics and the EU Guidelines for Trustworthy AI.

---

## ðŸ§° Tools & Technologies

- Google Colab / Jupyter Notebook  
- Python 3.11  
- AI Fairness 360 (aif360)  
- scikit-learn  
- pandas, matplotlib, seaborn  
- COMPAS Dataset (via AIF360)


## ðŸ“Š Fairness Metrics Used

- **Disparate Impact**  
- **Equal Opportunity Difference**  
- **False Positive Rate Difference**

These help evaluate whether the AI system treats privileged (White) and unprivileged (Black) groups fairly.

